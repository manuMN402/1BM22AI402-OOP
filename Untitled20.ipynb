{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqbALGOJsR28ey75i4izCJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuMN402/1BM22AI402-OOP/blob/main/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "class RLStopGoEnvironment:\n",
        "    def _init_(self, num_samples=10, image_shape=(64, 64, 3)):\n",
        "        self.num_samples = num_samples\n",
        "        self.image_shape = image_shape\n",
        "        self.images = np.random.random(size=(num_samples,) + image_shape)\n",
        "        self.labels = np.random.randint(2, size=num_samples)\n",
        "        self.current_step = 0\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "    def step(self, action):\n",
        "        reward = self.labels[self.current_step]\n",
        "        self.current_step += 1\n",
        "        done = self.current_step == self.num_samples\n",
        "        return self.images[self.current_step - 1], reward, done\n",
        "class QLearningAgent:\n",
        "    def _init_(self, state_shape, num_actions):\n",
        "        self.model = models.Sequential([\n",
        "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=state_shape),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(num_actions, activation='linear')\n",
        "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    def choose_action(self, state):\n",
        "        epsilon = 0.1\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.model.output_shape[1])\n",
        "        else:\n",
        "            return np.argmax(self.model.predict(state[np.newaxis, ...]))\n",
        "    def train(self, state, action, reward, next_state):\n",
        "        target = reward + 0.9 * np.max(self.model.predict(next_state[np.newaxis, ...]))\n",
        "        target_f = self.model.predict(state[np.newaxis, ...])\n",
        "        target_f[0, action] = target\n",
        "        self.model.fit(state[np.newaxis, ...], target_f, epochs=1, verbose=0)\n",
        "num_episodes = 50\n",
        "env = RLStopGoEnvironment()\n",
        "state_shape = env.image_shape\n",
        "num_actions = 2\n",
        "agent = QLearningAgent(state_shape, num_actions)\n",
        "for episode in range(num_episodes):\n",
        "    state = env.images[0]\n",
        "    env.reset()\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        total_reward += reward\n",
        "        agent.train(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
        "new_state = env.images[0]\n",
        "action = agent.choose_action(new_state)\n",
        "decision = \"go\" if action == 1 else \"stop\"\n",
        "print(f\"Decision: {decision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "XPHvnpYdxSsx",
        "outputId": "b3c5dc52-6ef4-4bab-ecc9-81e067fe3d66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'[' was never closed (<ipython-input-2-01945966d725>, line 20)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-01945966d725>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    self.model = models.Sequential([\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '[' was never closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "LeW3pAgTxO_P"
      }
    }
  ]
}